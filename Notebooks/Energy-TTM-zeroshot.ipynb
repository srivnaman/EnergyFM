{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "\n",
    "\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "from time import time\n",
    "import math \n",
    "import tempfile \n",
    "import torch \n",
    "import pickle \n",
    "import logging \n",
    "import warnings\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, set_seed, EarlyStoppingCallback\n",
    "from torch.utils.data import ConcatDataset, Dataset, DataLoader\n",
    "\n",
    "\n",
    "from tsfm_public.models.tinytimemixer.configuration_tinytimemixer import TinyTimeMixerConfig\n",
    "# from tinytimemixer.modeling_tinytimemixer import TinyTimeMixerForPrediction\n",
    "from tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\n",
    "from tsfm_public.toolkit.dataset import PretrainDFDataset, ForecastDFDataset\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n",
    "from tsfm_public.toolkit.util import select_by_index\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metrics used for evaluation\n",
    "def cal_cvrmse(pred, true, eps=1e-8):\n",
    "    pred = np.array(pred)\n",
    "    true = np.array(true)\n",
    "    return np.power(np.square(pred - true).sum() / pred.shape[0], 0.5) / (true.sum() / pred.shape[0] + eps)\n",
    "\n",
    "def cal_mae(pred, true):\n",
    "    pred = np.array(pred)\n",
    "    true = np.array(true)\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "def cal_nrmse(pred, true, eps=1e-8):\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    M = len(true) // 24\n",
    "    y_bar = np.mean(true)\n",
    "    NRMSE = 100 * (1/ (y_bar+eps)) * np.sqrt((1 / (24 * M)) * np.sum((true - pred) ** 2))\n",
    "    return NRMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def standardize_series(series, eps=1e-8):\n",
    "    mean = np.mean(series)\n",
    "    std = np.std(series)\n",
    "    standardized_series = (series - mean) / (std+eps)\n",
    "    return standardized_series, mean, std\n",
    "\n",
    "def unscale_predictions(predictions, mean, std, eps=1e-8):\n",
    "    return predictions * (std+eps) + mean\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, backcast_length, forecast_length, stride=1):\n",
    "        # Standardize the time series data\n",
    "        self.data, self.mean, self.std = standardize_series(data)\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.backcast_length - self.forecast_length) // self.stride + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.stride\n",
    "        x = self.data[start_index : start_index + self.backcast_length]\n",
    "        y = self.data[start_index + self.backcast_length : start_index + self.backcast_length + self.forecast_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(args):\n",
    "\n",
    "    config = TinyTimeMixerConfig(\n",
    "        context_length=args[\"context_length\"],\n",
    "        patch_length=args[\"patch_length\"],\n",
    "        num_input_channels=args[\"num_input_channels\"],\n",
    "        patch_stride=args[\"patch_stride\"],\n",
    "        d_model=args[\"d_model\"],\n",
    "        num_layers=args[\"num_layers\"],\n",
    "        expansion_factor=args[\"expansion_factor\"],\n",
    "        dropout=args[\"dropout\"],\n",
    "        head_dropout=args[\"head_dropout\"],\n",
    "        mode=args[\"mode\"][0],\n",
    "        scaling=args[\"scaling\"],\n",
    "        prediction_length=args[\"prediction_length\"],\n",
    "        is_scaling=args[\"is_scaling\"],\n",
    "        gated_attn=args[\"gated_attn\"],\n",
    "        norm_mlp=args[\"norm_mlp\"],\n",
    "        self_attn=args[\"self_attn\"],\n",
    "        self_attn_heads=args[\"self_attn_heads\"],\n",
    "        use_positional_encoding=args[\"use_positional_encoding\"],\n",
    "        positional_encoding_type=args[\"positional_encoding_type\"],\n",
    "        loss=args[\"loss\"],\n",
    "        init_std=args[\"init_std\"],\n",
    "        post_init=args[\"post_init\"],\n",
    "        norm_eps=args[\"norm_eps\"],\n",
    "        adaptive_patching_levels=args[\"adaptive_patching_levels\"],\n",
    "        resolution_prefix_tuning=args[\"resolution_prefix_tuning\"],\n",
    "        frequency_token_vocab_size=args[\"frequency_token_vocab_size\"],\n",
    "        distribution_output=args[\"distribution_output\"],\n",
    "        num_parallel_samples=args[\"num_parallel_samples\"],\n",
    "        decoder_num_layers=args[\"decoder_num_layers\"],\n",
    "        decoder_d_model=args[\"decoder_d_model\"],\n",
    "        decoder_adaptive_patching_levels=args[\"decoder_adaptive_patching_levels\"],\n",
    "        decoder_raw_residual=args[\"decoder_raw_residual\"],\n",
    "        decoder_mode=args[\"decoder_mode\"],\n",
    "        use_decoder=args[\"use_decoder\"],\n",
    "        enable_forecast_channel_mixing=args[\"enable_forecast_channel_mixing\"],\n",
    "        fcm_gated_attn=args[\"fcm_gated_attn\"],\n",
    "        fcm_context_length=args[\"fcm_context_length\"],\n",
    "        fcm_use_mixer=args[\"fcm_use_mixer\"],\n",
    "        fcm_mix_layers=args[\"fcm_mix_layers\"],\n",
    "        fcm_prepend_past=args[\"fcm_prepend_past\"], \n",
    "        init_linear=args[\"init_linear\"],\n",
    "        init_embed=args[\"init_embed\"],\n",
    "\n",
    "    )\n",
    "\n",
    "    pretraining_model = TinyTimeMixerForPrediction(config)\n",
    "    return pretraining_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    args,\n",
    "    model,\n",
    "    criterion,\n",
    "    dataset_path,\n",
    "    result_path,\n",
    "    device,\n",
    "    target_buildings=\"BR02\"   # can be \"BR02\", [\"BR02\",\"BR03\"], or \"all\"\n",
    "):\n",
    "\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    # Loop over parquet files\n",
    "    for file_name in os.listdir(dataset_path):\n",
    "\n",
    "        if not file_name.endswith(\".parquet\"):\n",
    "            continue\n",
    "\n",
    "        file_id = file_name.replace(\".parquet\", \"\")\n",
    "        file_path = os.path.join(dataset_path, file_name)\n",
    "\n",
    "        print(f\"Testing file: {file_id}\")\n",
    "\n",
    "        # Load parquet\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Determine target buildings\n",
    "        # ---------------------------------------------------\n",
    "        if target_buildings == \"all\":\n",
    "            buildings_to_test = list(df.columns)\n",
    "\n",
    "        elif isinstance(target_buildings, str):\n",
    "            buildings_to_test = [target_buildings]\n",
    "\n",
    "        elif isinstance(target_buildings, list):\n",
    "            buildings_to_test = target_buildings\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"target_buildings must be 'all', a string, or list\")\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Test each building column\n",
    "        # ---------------------------------------------------\n",
    "        for building_col in buildings_to_test:\n",
    "\n",
    "            if building_col not in df.columns:\n",
    "                print(f\" {building_col} not found in {file_id}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n   ▶ Building: {building_col}\")\n",
    "\n",
    "            # Extract series\n",
    "            energy_data = df[building_col].values.astype(np.float32)\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Fill NaNs with Median\n",
    "            # ---------------------------------------------------\n",
    "            nan_count = np.isnan(energy_data).sum()\n",
    "\n",
    "            if nan_count > 0:\n",
    "                median_val = np.nanmedian(energy_data)\n",
    "                energy_data = np.where(\n",
    "                    np.isnan(energy_data),\n",
    "                    median_val,\n",
    "                    energy_data\n",
    "                )\n",
    "\n",
    "                print(f\"  Filled {nan_count} NaNs with median={median_val:.4f}\")\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Check minimum length\n",
    "            # ---------------------------------------------------\n",
    "            min_required = args[\"context_length\"] + args[\"prediction_length\"]\n",
    "\n",
    "            if len(energy_data) < min_required:\n",
    "                print(\"   Too short, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Dataset creation\n",
    "            dataset = TimeSeriesDataset(\n",
    "                energy_data,\n",
    "                args[\"context_length\"],\n",
    "                args[\"prediction_length\"],\n",
    "                args[\"patch_stride\"]\n",
    "            )\n",
    "\n",
    "            if len(dataset) == 0:\n",
    "                print(\"   No samples, skipping...\")\n",
    "                continue\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            val_losses = []\n",
    "            y_true_test = []\n",
    "            y_pred_test = []\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Testing loop\n",
    "            # ---------------------------------------------------\n",
    "            for x_test, y_test in DataLoader(dataset, batch_size=1):\n",
    "\n",
    "                x_test = x_test.unsqueeze(-1).to(device)\n",
    "                y_test = y_test.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(x_test)\n",
    "                    forecast = output.prediction_outputs.squeeze(-1)\n",
    "\n",
    "                    loss = criterion(forecast, y_test)\n",
    "\n",
    "                    if torch.isnan(loss):\n",
    "                        continue\n",
    "\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "                    y_true_test.append(y_test.cpu().numpy())\n",
    "                    y_pred_test.append(forecast.cpu().numpy())\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            #  Skip empty results\n",
    "            # ---------------------------------------------------\n",
    "            if len(y_true_test) == 0:\n",
    "                print(\"   No predictions collected, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Combine predictions\n",
    "            y_true = np.concatenate(y_true_test, axis=0)\n",
    "            y_pred = np.concatenate(y_pred_test, axis=0)\n",
    "\n",
    "            # Unscale\n",
    "            y_pred_unscaled = unscale_predictions(y_pred, dataset.mean, dataset.std)\n",
    "            y_true_unscaled = unscale_predictions(y_true, dataset.mean, dataset.std)\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            #  Metrics\n",
    "            # ---------------------------------------------------\n",
    "            cvrmse = cal_cvrmse(y_pred_unscaled, y_true_unscaled)\n",
    "            nrmse  = cal_nrmse(y_pred_unscaled, y_true_unscaled)\n",
    "            mae    = cal_mae(y_pred_unscaled, y_true_unscaled)\n",
    "\n",
    "            avg_loss = np.mean(val_losses)\n",
    "\n",
    "            print(f\"  CVRMSE={cvrmse:.4f}, NRMSE={nrmse:.4f}, MAE={mae:.4f}\")\n",
    "\n",
    "            # Save row\n",
    "            res.append([\n",
    "                file_id,\n",
    "                building_col,\n",
    "                cvrmse,\n",
    "                nrmse,\n",
    "                mae,\n",
    "                avg_loss\n",
    "            ])\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Save Results\n",
    "    # ---------------------------------------------------\n",
    "    columns = [\"Dataset\", \"Building\", \"CVRMSE\", \"NRMSE\", \"MAE\", \"Avg_Test_Loss\"]\n",
    "\n",
    "    result_df = pd.DataFrame(res, columns=columns)\n",
    "\n",
    "    result_csv = os.path.join(result_path, \"test_results.csv\")\n",
    "    result_df.to_csv(result_csv, index=False)\n",
    "\n",
    "    print(\"\\n Testing complete!\")\n",
    "    print(\" Results saved at:\", result_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:p-544723:t-140688640546624:modeling_tinytimemixer.py:__init__:Disabling adaptive patching at level 2. Either increase d_model or reduce adaptive_patching_levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's parameter count is: 28858\n",
      "Testing file: Bareilly-1H\n",
      "\n",
      "   ▶ Building: BR02\n",
      "  Filled 9984 NaNs with median=0.1660\n",
      "  CVRMSE=0.0974, NRMSE=236.3778, MAE=0.0428\n",
      "Testing file: Mathura-1H\n",
      " BR02 not found in Mathura-1H, skipping...\n",
      "\n",
      " Testing complete!\n",
      " Results saved at: test_results_zeroshot/test_results.csv\n"
     ]
    }
   ],
   "source": [
    "config_file = '../Energy-TTM/config/tinyTimeMixers.json'\n",
    "with open(config_file, 'r') as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "# check device \n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# define TTMs model\n",
    "model = model_config(args).to(device)\n",
    "model.load_state_dict(torch.load('../Energy-TTM/Weights/energy_ttm.pth'))\n",
    "\n",
    "# model's parameters\n",
    "param = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Model's parameter count is:\", param)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# print(args.result_path)\n",
    "# training the model and save best parameters\n",
    "test(args=args, model=model, criterion=criterion,dataset_path=\"../Dataset/Forecasting\",result_path=\"test_results_zeroshot\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy-ttm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
