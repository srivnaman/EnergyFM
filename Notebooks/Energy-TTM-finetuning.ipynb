{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Fine-Tuning EnergyTTM\n",
    "\n",
    "This notebook performs **fine-tuning of the pretrained EnergyTTM model** on downstream energy forecasting data to adapt it to specific buildings or datasets.\n",
    "\n",
    "The workflow includes:\n",
    "- Loading pretrained EnergyTTM weights\n",
    "- Preparing and cleaning time-series energy data\n",
    "- Training the model using supervised forecasting objectives\n",
    "- Evaluating performance using standard forecasting metrics\n",
    "\n",
    "The objective is to improve task-specific performance by adapting EnergyTTM to the target dataset while leveraging pretrained representations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "from time import time\n",
    "import math \n",
    "import tempfile \n",
    "import torch \n",
    "import pickle \n",
    "import logging \n",
    "import warnings\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, set_seed, EarlyStoppingCallback\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from tsfm_public.models.tinytimemixer.configuration_tinytimemixer import TinyTimeMixerConfig\n",
    "from tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\n",
    "from tsfm_public.toolkit.util import select_by_index\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "This section defines the performance metrics used to evaluate forecasting quality.\n",
    "\n",
    "The following metrics are computed:\n",
    "\n",
    "- **CVRMSE (Coefficient of Variation of RMSE)**  \n",
    "  Measures normalized root mean squared error relative to the mean of the true values.  \n",
    "  Useful for energy forecasting benchmarks where scale normalization is required.\n",
    "\n",
    "- **MAE (Mean Absolute Error)**  \n",
    "  Computes the average absolute difference between predictions and ground truth.  \n",
    "  Provides an interpretable measure of average forecast deviation.\n",
    "\n",
    "- **NRMSE (Normalized RMSE, %)**  \n",
    "  Root mean squared error normalized by the mean load and expressed as a percentage.  \n",
    "  Particularly suitable for daily energy series (24-hour structure assumed).\n",
    "\n",
    "These metrics together evaluate both absolute and scale-normalized forecasting performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metrics used for evaluation\n",
    "def cal_cvrmse(pred, true, eps=1e-8):\n",
    "    pred = np.array(pred)\n",
    "    true = np.array(true)\n",
    "    return np.power(np.square(pred - true).sum() / pred.shape[0], 0.5) / (true.sum() / pred.shape[0] + eps)\n",
    "\n",
    "def cal_mae(pred, true):\n",
    "    pred = np.array(pred)\n",
    "    true = np.array(true)\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "def cal_nrmse(pred, true, eps=1e-8):\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    M = len(true) // 24\n",
    "    y_bar = np.mean(true)\n",
    "    NRMSE = 100 * (1/ (y_bar+eps)) * np.sqrt((1 / (24 * M)) * np.sum((true - pred) ** 2))\n",
    "    return NRMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Series Dataset & Scaling\n",
    "\n",
    "This section defines preprocessing and dataset utilities for supervised forecasting.\n",
    "\n",
    "### ðŸ”¹ Standardization\n",
    "- The time series is normalized using z-score scaling:\n",
    "  $\n",
    "  x' = \\frac{x - \\mu}{\\sigma}\n",
    "  $\n",
    "- Mean and standard deviation are stored to later **unscale predictions** back to the original energy units.\n",
    "\n",
    "### ðŸ”¹ Sliding Window Dataset\n",
    "`TimeSeriesDataset` constructs supervised training samples using a rolling window approach:\n",
    "\n",
    "- **Backcast length** â†’ historical input window  \n",
    "- **Forecast length** â†’ prediction horizon  \n",
    "- **Stride** â†’ step size between consecutive windows  \n",
    "\n",
    "Each sample returns:\n",
    "- `x` â†’ past sequence (input)\n",
    "- `y` â†’ future sequence (target)\n",
    "\n",
    "This enables efficient training for sequence-to-sequence forecasting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def standardize_series(series, eps=1e-8):\n",
    "    mean = np.mean(series)\n",
    "    std = np.std(series)\n",
    "    standardized_series = (series - mean) / (std+eps)\n",
    "    return standardized_series, mean, std\n",
    "\n",
    "def unscale_predictions(predictions, mean, std, eps=1e-8):\n",
    "    return predictions * (std+eps) + mean\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, backcast_length, forecast_length, stride=1):\n",
    "        # Standardize the time series data\n",
    "        self.data, self.mean, self.std = standardize_series(data)\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.backcast_length - self.forecast_length) // self.stride + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.stride\n",
    "        x = self.data[start_index : start_index + self.backcast_length]\n",
    "        y = self.data[start_index + self.backcast_length : start_index + self.backcast_length + self.forecast_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the TinyTimeMixer Model\n",
    "\n",
    "Here, we initialize the **TinyTimeMixer forecasting model** using a structured configuration dictionary.\n",
    "\n",
    "### ðŸ”¹ What We Do\n",
    "- Create a `TinyTimeMixerConfig` from the provided `args`\n",
    "- Set architectural and training parameters (context length, prediction length, layers, attention, patching, decoder options, etc.)\n",
    "- Instantiate `TinyTimeMixerForPrediction` with this configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(args):\n",
    "\n",
    "    config = TinyTimeMixerConfig(\n",
    "        context_length=args[\"context_length\"],\n",
    "        patch_length=args[\"patch_length\"],\n",
    "        num_input_channels=args[\"num_input_channels\"],\n",
    "        patch_stride=args[\"patch_stride\"],\n",
    "        d_model=args[\"d_model\"],\n",
    "        num_layers=args[\"num_layers\"],\n",
    "        expansion_factor=args[\"expansion_factor\"],\n",
    "        dropout=args[\"dropout\"],\n",
    "        head_dropout=args[\"head_dropout\"],\n",
    "        mode=args[\"mode\"][0],\n",
    "        scaling=args[\"scaling\"],\n",
    "        prediction_length=args[\"prediction_length\"],\n",
    "        is_scaling=args[\"is_scaling\"],\n",
    "        gated_attn=args[\"gated_attn\"],\n",
    "        norm_mlp=args[\"norm_mlp\"],\n",
    "        self_attn=args[\"self_attn\"],\n",
    "        self_attn_heads=args[\"self_attn_heads\"],\n",
    "        use_positional_encoding=args[\"use_positional_encoding\"],\n",
    "        positional_encoding_type=args[\"positional_encoding_type\"],\n",
    "        loss=args[\"loss\"],\n",
    "        init_std=args[\"init_std\"],\n",
    "        post_init=args[\"post_init\"],\n",
    "        norm_eps=args[\"norm_eps\"],\n",
    "        adaptive_patching_levels=args[\"adaptive_patching_levels\"],\n",
    "        resolution_prefix_tuning=args[\"resolution_prefix_tuning\"],\n",
    "        frequency_token_vocab_size=args[\"frequency_token_vocab_size\"],\n",
    "        distribution_output=args[\"distribution_output\"],\n",
    "        num_parallel_samples=args[\"num_parallel_samples\"],\n",
    "        decoder_num_layers=args[\"decoder_num_layers\"],\n",
    "        decoder_d_model=args[\"decoder_d_model\"],\n",
    "        decoder_adaptive_patching_levels=args[\"decoder_adaptive_patching_levels\"],\n",
    "        decoder_raw_residual=args[\"decoder_raw_residual\"],\n",
    "        decoder_mode=args[\"decoder_mode\"],\n",
    "        use_decoder=args[\"use_decoder\"],\n",
    "        enable_forecast_channel_mixing=args[\"enable_forecast_channel_mixing\"],\n",
    "        fcm_gated_attn=args[\"fcm_gated_attn\"],\n",
    "        fcm_context_length=args[\"fcm_context_length\"],\n",
    "        fcm_use_mixer=args[\"fcm_use_mixer\"],\n",
    "        fcm_mix_layers=args[\"fcm_mix_layers\"],\n",
    "        fcm_prepend_past=args[\"fcm_prepend_past\"], \n",
    "        init_linear=args[\"init_linear\"],\n",
    "        init_embed=args[\"init_embed\"],\n",
    "\n",
    "    )\n",
    "\n",
    "    pretraining_model = TinyTimeMixerForPrediction(config)\n",
    "    return pretraining_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MH01</th>\n",
       "      <th>MH02</th>\n",
       "      <th>MH03</th>\n",
       "      <th>MH06</th>\n",
       "      <th>MH07</th>\n",
       "      <th>MH08</th>\n",
       "      <th>MH09</th>\n",
       "      <th>MH10</th>\n",
       "      <th>MH11</th>\n",
       "      <th>MH12</th>\n",
       "      <th>...</th>\n",
       "      <th>MH36</th>\n",
       "      <th>MH37</th>\n",
       "      <th>MH38</th>\n",
       "      <th>MH39</th>\n",
       "      <th>MH41</th>\n",
       "      <th>MH42</th>\n",
       "      <th>MH43</th>\n",
       "      <th>MH45</th>\n",
       "      <th>MH46</th>\n",
       "      <th>MH47</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-05-01 00:00:00</th>\n",
       "      <td>0.262</td>\n",
       "      <td>2.039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.437</td>\n",
       "      <td>1.915</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.283</td>\n",
       "      <td>2.484</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.342</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 01:00:00</th>\n",
       "      <td>0.309</td>\n",
       "      <td>2.036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.406</td>\n",
       "      <td>1.878</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.235</td>\n",
       "      <td>2.438</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.360</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 02:00:00</th>\n",
       "      <td>0.315</td>\n",
       "      <td>2.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.404</td>\n",
       "      <td>1.822</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.243</td>\n",
       "      <td>2.332</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.371</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 03:00:00</th>\n",
       "      <td>0.318</td>\n",
       "      <td>2.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470</td>\n",
       "      <td>1.844</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.272</td>\n",
       "      <td>2.322</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.314</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 04:00:00</th>\n",
       "      <td>0.324</td>\n",
       "      <td>2.045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.479</td>\n",
       "      <td>1.786</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.333</td>\n",
       "      <td>2.313</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 19:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 20:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 21:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 22:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 23:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15888 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      MH01   MH02  MH03   MH06   MH07   MH08   MH09   MH10  \\\n",
       "Timestamp                                                                    \n",
       "2019-05-01 00:00:00  0.262  2.039   NaN  0.437  1.915  0.302  0.283  2.484   \n",
       "2019-05-01 01:00:00  0.309  2.036   NaN  0.406  1.878  0.321  0.235  2.438   \n",
       "2019-05-01 02:00:00  0.315  2.018   NaN  0.404  1.822  0.321  0.243  2.332   \n",
       "2019-05-01 03:00:00  0.318  2.018   NaN  0.470  1.844  0.333  0.272  2.322   \n",
       "2019-05-01 04:00:00  0.324  2.045   NaN  0.479  1.786  0.322  0.333  2.313   \n",
       "...                    ...    ...   ...    ...    ...    ...    ...    ...   \n",
       "2021-02-20 19:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2021-02-20 20:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2021-02-20 21:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2021-02-20 22:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2021-02-20 23:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "                      MH11   MH12  ...  MH36  MH37  MH38  MH39   MH41   MH42  \\\n",
       "Timestamp                          ...                                         \n",
       "2019-05-01 00:00:00  0.788  0.342  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "2019-05-01 01:00:00  0.799  0.360  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "2019-05-01 02:00:00  0.815  0.371  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "2019-05-01 03:00:00  0.822  0.314  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "2019-05-01 04:00:00  0.592  0.250  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "...                    ...    ...  ...   ...   ...   ...   ...    ...    ...   \n",
       "2021-02-20 19:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.153  0.195   \n",
       "2021-02-20 20:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.112  0.210   \n",
       "2021-02-20 21:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.097  0.130   \n",
       "2021-02-20 22:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.037  0.097   \n",
       "2021-02-20 23:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.018  0.035   \n",
       "\n",
       "                      MH43   MH45  MH46  MH47  \n",
       "Timestamp                                      \n",
       "2019-05-01 00:00:00    NaN    NaN   NaN   NaN  \n",
       "2019-05-01 01:00:00    NaN    NaN   NaN   NaN  \n",
       "2019-05-01 02:00:00    NaN    NaN   NaN   NaN  \n",
       "2019-05-01 03:00:00    NaN    NaN   NaN   NaN  \n",
       "2019-05-01 04:00:00    NaN    NaN   NaN   NaN  \n",
       "...                    ...    ...   ...   ...  \n",
       "2021-02-20 19:00:00  0.282  0.140   NaN   NaN  \n",
       "2021-02-20 20:00:00  0.137  0.129   NaN   NaN  \n",
       "2021-02-20 21:00:00  0.202  0.179   NaN   NaN  \n",
       "2021-02-20 22:00:00  0.117  0.145   NaN   NaN  \n",
       "2021-02-20 23:00:00  0.011  0.073   NaN   NaN  \n",
       "\n",
       "[15888 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.read_parquet('../Dataset/Forecasting/Mathura-1H.parquet')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ› ï¸ Helper Functions for the Evaluation Pipeline\n",
    "\n",
    "These helper functions modularize the testing workflow to improve readability, reusability, and debugging. They handle building selection, data cleaning, dataset preparation, inference, and metric computation in clearly separated steps. This structured design keeps the main evaluation loop clean and maintainable.\n",
    "\n",
    "- **Building Selection** â€“ Identify which building columns to evaluate (`\"all\"`, single, or multiple).\n",
    "- **Data Cleaning** â€“ Replace missing values using median imputation.\n",
    "- **Dataset Validation** â€“ Ensure sufficient sequence length and construct sliding-window samples.\n",
    "- **Inference Execution** â€“ Run model forward passes and collect predictions.\n",
    "- **Metric Evaluation** â€“ Unscale outputs and compute CVRMSE, NRMSE, and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buildings_to_test(df, target_buildings):\n",
    "    if target_buildings == \"all\":\n",
    "        return list(df.columns)\n",
    "    elif isinstance(target_buildings, str):\n",
    "        return [target_buildings]\n",
    "    elif isinstance(target_buildings, list):\n",
    "        return target_buildings\n",
    "    else:\n",
    "        raise ValueError(\"target_buildings must be 'all', a string, or list\")\n",
    "\n",
    "\n",
    "def clean_series(energy_data):\n",
    "    nan_count = np.isnan(energy_data).sum()\n",
    "    if nan_count > 0:\n",
    "        median_val = np.nanmedian(energy_data)\n",
    "        energy_data = np.where(np.isnan(energy_data), median_val, energy_data)\n",
    "        print(f\"  Filled {nan_count} NaNs with median={median_val:.4f}\")\n",
    "    return energy_data\n",
    "\n",
    "\n",
    "def create_dataset_if_valid(energy_data, args):\n",
    "    min_required = args[\"context_length\"] + args[\"prediction_length\"]\n",
    "\n",
    "    if len(energy_data) < min_required:\n",
    "        print(\"   Too short, skipping...\")\n",
    "        return None\n",
    "\n",
    "    dataset = TimeSeriesDataset(\n",
    "        energy_data,\n",
    "        args[\"context_length\"],\n",
    "        args[\"prediction_length\"],\n",
    "        args[\"patch_stride\"]\n",
    "    )\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"   No samples, skipping...\")\n",
    "        return None\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def run_inference(model, dataset, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    val_losses = []\n",
    "    y_true_test = []\n",
    "    y_pred_test = []\n",
    "\n",
    "    for x_test, y_test in DataLoader(dataset, batch_size=1):\n",
    "        x_test = x_test.unsqueeze(-1).to(device)\n",
    "        y_test = y_test.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(x_test)\n",
    "            forecast = output.prediction_outputs.squeeze(-1)\n",
    "\n",
    "            loss = criterion(forecast, y_test)\n",
    "            if torch.isnan(loss):\n",
    "                continue\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            y_true_test.append(y_test.cpu().numpy())\n",
    "            y_pred_test.append(forecast.cpu().numpy())\n",
    "\n",
    "    if len(y_true_test) == 0:\n",
    "        return None\n",
    "\n",
    "    y_true = np.concatenate(y_true_test, axis=0)\n",
    "    y_pred = np.concatenate(y_pred_test, axis=0)\n",
    "\n",
    "    return y_true, y_pred, np.mean(val_losses)\n",
    "\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, dataset):\n",
    "    y_pred_unscaled = unscale_predictions(y_pred, dataset.mean, dataset.std)\n",
    "    y_true_unscaled = unscale_predictions(y_true, dataset.mean, dataset.std)\n",
    "\n",
    "    cvrmse = cal_cvrmse(y_pred_unscaled, y_true_unscaled)\n",
    "    nrmse  = cal_nrmse(y_pred_unscaled, y_true_unscaled)\n",
    "    mae    = cal_mae(y_pred_unscaled, y_true_unscaled)\n",
    "\n",
    "    return cvrmse, nrmse, mae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(args, model, criterion, dataset_path, result_path, device, target_buildings=\"BR02\"):\n",
    "\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    res = []\n",
    "\n",
    "    for file_name in os.listdir(dataset_path):\n",
    "\n",
    "        if not file_name.endswith(\".parquet\"):\n",
    "            continue\n",
    "\n",
    "        file_id = file_name.replace(\".parquet\", \"\")\n",
    "        file_path = os.path.join(dataset_path, file_name)\n",
    "\n",
    "        print(f\"\\nTesting file: {file_id}\")\n",
    "\n",
    "        df = pd.read_parquet(file_path)\n",
    "        buildings_to_test = get_buildings_to_test(df, target_buildings)\n",
    "\n",
    "        for building_col in buildings_to_test:\n",
    "\n",
    "            if building_col not in df.columns:\n",
    "                print(f\" {building_col} not found, skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"   â–¶ Building: {building_col}\")\n",
    "\n",
    "            energy_data = df[building_col].values.astype(np.float32)\n",
    "            energy_data = clean_series(energy_data)\n",
    "\n",
    "            dataset = create_dataset_if_valid(energy_data, args)\n",
    "            if dataset is None:\n",
    "                continue\n",
    "\n",
    "            inference_output = run_inference(model, dataset, criterion, device)\n",
    "            if inference_output is None:\n",
    "                print(\"   No predictions collected, skipping...\")\n",
    "                continue\n",
    "\n",
    "            y_true, y_pred, avg_loss = inference_output\n",
    "            cvrmse, nrmse, mae = evaluate_predictions(y_true, y_pred, dataset)\n",
    "\n",
    "            print(f\"   CVRMSE={cvrmse:.4f}, NRMSE={nrmse:.4f}, MAE={mae:.4f}\")\n",
    "\n",
    "            res.append([file_id, building_col, cvrmse, nrmse, mae, avg_loss])\n",
    "\n",
    "    columns = [\"Dataset\", \"Building\", \"CVRMSE\", \"NRMSE\", \"MAE\", \"Avg_Test_Loss\"]\n",
    "    result_df = pd.DataFrame(res, columns=columns)\n",
    "\n",
    "    result_csv = os.path.join(result_path, \"test_results.csv\")\n",
    "    result_df.to_csv(result_csv, index=False)\n",
    "\n",
    "    print(\"\\nTesting complete!\")\n",
    "    print(\"Results saved at:\", result_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Helper Functions\n",
    "\n",
    "These helper functions modularize the fine-tuning pipeline to keep training logic clean, reusable, and easy to extend. They separate data preparation, batch-level training, epoch orchestration, and checkpoint management into independent components.\n",
    "\n",
    "- **Series Cleaning & Validation** â€“ Impute missing values, verify minimum sequence length, and construct a sliding-window dataset.\n",
    "- **Batch Training Step** â€“ Perform forward pass, compute loss, backpropagate, and update model weights.\n",
    "- **Epoch Training Loop** â€“ Iterate over parquet files and building columns to aggregate training losses.\n",
    "- **Checkpoint Saving** â€“ Track the best epoch loss and save the best-performing model state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_validate_series(energy_data, args):\n",
    "    # Fill NaNs with median\n",
    "    if np.isnan(energy_data).any():\n",
    "        energy_data = np.nan_to_num(\n",
    "            energy_data,\n",
    "            nan=np.nanmedian(energy_data)\n",
    "        )\n",
    "\n",
    "    min_required = args[\"context_length\"] + args[\"prediction_length\"]\n",
    "\n",
    "    if len(energy_data) < min_required:\n",
    "        return None\n",
    "\n",
    "    dataset = TimeSeriesDataset(\n",
    "        energy_data,\n",
    "        args[\"context_length\"],\n",
    "        args[\"prediction_length\"],\n",
    "        args[\"patch_stride\"],\n",
    "    )\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        return None\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def train_one_loader(model, loader, criterion, optimizer, device):\n",
    "    batch_losses = []\n",
    "\n",
    "    for x, y in loader:\n",
    "\n",
    "        x = x.unsqueeze(-1).to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(x)\n",
    "        forecast = output.prediction_outputs.squeeze(-1)\n",
    "\n",
    "        loss = criterion(forecast, y)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "    return batch_losses\n",
    "\n",
    "\n",
    "def train_one_epoch(args, model, criterion, optimizer, dataset_path, device):\n",
    "    epoch_losses = []\n",
    "\n",
    "    for file_name in os.listdir(dataset_path):\n",
    "\n",
    "        if not file_name.endswith(\".parquet\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(dataset_path, file_name)\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        for building_col in df.columns:\n",
    "\n",
    "            if not pd.api.types.is_numeric_dtype(df[building_col]):\n",
    "                continue\n",
    "\n",
    "            energy_data = df[building_col].values.astype(np.float32)\n",
    "\n",
    "            dataset = clean_and_validate_series(energy_data, args)\n",
    "            if dataset is None:\n",
    "                continue\n",
    "\n",
    "            loader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=args[\"batch_size\"],\n",
    "                shuffle=True\n",
    "            )\n",
    "\n",
    "            batch_losses = train_one_loader(\n",
    "                model, loader, criterion, optimizer, device\n",
    "            )\n",
    "\n",
    "            epoch_losses.extend(batch_losses)\n",
    "\n",
    "    return epoch_losses\n",
    "\n",
    "\n",
    "def save_best_model(model, avg_epoch_loss, best_loss, save_path):\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        ckpt_path = os.path.join(save_path, \"best_model.pth\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\" âœ“ Saved best model to {ckpt_path}\")\n",
    "    return best_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def finetune(\n",
    "    args,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataset_path,\n",
    "    save_path,\n",
    "    device,\n",
    "):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    print(\"\\n Starting fine-tuning...\")\n",
    "\n",
    "    for epoch in range(args[\"num_epochs\"]):\n",
    "\n",
    "        print(f\"\\n Epoch [{epoch+1}/{args['num_epochs']}]\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        epoch_losses = train_one_epoch(\n",
    "            args, model, criterion, optimizer, dataset_path, device\n",
    "        )\n",
    "\n",
    "        if len(epoch_losses) == 0:\n",
    "            print(\" No valid batches in this epoch.\")\n",
    "            continue\n",
    "\n",
    "        avg_epoch_loss = np.mean(epoch_losses)\n",
    "        print(f\" Avg Train Loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "        best_loss = save_best_model(\n",
    "            model, avg_epoch_loss, best_loss, save_path\n",
    "        )\n",
    "\n",
    "    print(\"\\n Fine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/naman/Final-EFM/EnergyFM/Notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:p-2077266:t-139847142426432:modeling_tinytimemixer.py:__init__:Disabling adaptive patching at level 2. Either increase d_model or reduce adaptive_patching_levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting fine-tuning...\n",
      "\n",
      " Epoch [1/100]\n",
      " Avg Train Loss: 0.546966\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [2/100]\n",
      " Avg Train Loss: 0.559207\n",
      "\n",
      " Epoch [3/100]\n",
      " Avg Train Loss: 0.538352\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [4/100]\n",
      " Avg Train Loss: 0.540219\n",
      "\n",
      " Epoch [5/100]\n",
      " Avg Train Loss: 0.538008\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [6/100]\n",
      " Avg Train Loss: 0.540052\n",
      "\n",
      " Epoch [7/100]\n",
      " Avg Train Loss: 0.536389\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [8/100]\n",
      " Avg Train Loss: 0.535301\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [9/100]\n",
      " Avg Train Loss: 0.541710\n",
      "\n",
      " Epoch [10/100]\n",
      " Avg Train Loss: 0.541236\n",
      "\n",
      " Epoch [11/100]\n",
      " Avg Train Loss: 0.540401\n",
      "\n",
      " Epoch [12/100]\n",
      " Avg Train Loss: 0.542891\n",
      "\n",
      " Epoch [13/100]\n",
      " Avg Train Loss: 0.539718\n",
      "\n",
      " Epoch [14/100]\n",
      " Avg Train Loss: 0.538783\n",
      "\n",
      " Epoch [15/100]\n",
      " Avg Train Loss: 0.533906\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [16/100]\n",
      " Avg Train Loss: 0.534697\n",
      "\n",
      " Epoch [17/100]\n",
      " Avg Train Loss: 0.531871\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [18/100]\n",
      " Avg Train Loss: 0.533777\n",
      "\n",
      " Epoch [19/100]\n",
      " Avg Train Loss: 0.532894\n",
      "\n",
      " Epoch [20/100]\n",
      " Avg Train Loss: 0.534286\n",
      "\n",
      " Epoch [21/100]\n",
      " Avg Train Loss: 0.533992\n",
      "\n",
      " Epoch [22/100]\n",
      " Avg Train Loss: 0.537107\n",
      "\n",
      " Epoch [23/100]\n",
      " Avg Train Loss: 0.534348\n",
      "\n",
      " Epoch [24/100]\n",
      " Avg Train Loss: 0.539048\n",
      "\n",
      " Epoch [25/100]\n",
      " Avg Train Loss: 0.530354\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [26/100]\n",
      " Avg Train Loss: 0.532380\n",
      "\n",
      " Epoch [27/100]\n",
      " Avg Train Loss: 0.530194\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [28/100]\n",
      " Avg Train Loss: 0.532968\n",
      "\n",
      " Epoch [29/100]\n",
      " Avg Train Loss: 0.532831\n",
      "\n",
      " Epoch [30/100]\n",
      " Avg Train Loss: 0.529629\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [31/100]\n",
      " Avg Train Loss: 0.529033\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [32/100]\n",
      " Avg Train Loss: 0.538206\n",
      "\n",
      " Epoch [33/100]\n",
      " Avg Train Loss: 0.532796\n",
      "\n",
      " Epoch [34/100]\n",
      " Avg Train Loss: 0.532093\n",
      "\n",
      " Epoch [35/100]\n",
      " Avg Train Loss: 0.531882\n",
      "\n",
      " Epoch [36/100]\n",
      " Avg Train Loss: 0.529454\n",
      "\n",
      " Epoch [37/100]\n",
      " Avg Train Loss: 0.530825\n",
      "\n",
      " Epoch [38/100]\n",
      " Avg Train Loss: 0.532538\n",
      "\n",
      " Epoch [39/100]\n",
      " Avg Train Loss: 0.532287\n",
      "\n",
      " Epoch [40/100]\n",
      " Avg Train Loss: 0.529102\n",
      "\n",
      " Epoch [41/100]\n",
      " Avg Train Loss: 0.533272\n",
      "\n",
      " Epoch [42/100]\n",
      " Avg Train Loss: 0.533296\n",
      "\n",
      " Epoch [43/100]\n",
      " Avg Train Loss: 0.528920\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [44/100]\n",
      " Avg Train Loss: 0.525921\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [45/100]\n",
      " Avg Train Loss: 0.528747\n",
      "\n",
      " Epoch [46/100]\n",
      " Avg Train Loss: 0.524233\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [47/100]\n",
      " Avg Train Loss: 0.529401\n",
      "\n",
      " Epoch [48/100]\n",
      " Avg Train Loss: 0.531419\n",
      "\n",
      " Epoch [49/100]\n",
      " Avg Train Loss: 0.528746\n",
      "\n",
      " Epoch [50/100]\n",
      " Avg Train Loss: 0.530138\n",
      "\n",
      " Epoch [51/100]\n",
      " Avg Train Loss: 0.524934\n",
      "\n",
      " Epoch [52/100]\n",
      " Avg Train Loss: 0.526403\n",
      "\n",
      " Epoch [53/100]\n",
      " Avg Train Loss: 0.527449\n",
      "\n",
      " Epoch [54/100]\n",
      " Avg Train Loss: 0.532806\n",
      "\n",
      " Epoch [55/100]\n",
      " Avg Train Loss: 0.526627\n",
      "\n",
      " Epoch [56/100]\n",
      " Avg Train Loss: 0.531445\n",
      "\n",
      " Epoch [57/100]\n",
      " Avg Train Loss: 0.528100\n",
      "\n",
      " Epoch [58/100]\n",
      " Avg Train Loss: 0.528078\n",
      "\n",
      " Epoch [59/100]\n",
      " Avg Train Loss: 0.524914\n",
      "\n",
      " Epoch [60/100]\n",
      " Avg Train Loss: 0.532629\n",
      "\n",
      " Epoch [61/100]\n",
      " Avg Train Loss: 0.527762\n",
      "\n",
      " Epoch [62/100]\n",
      " Avg Train Loss: 0.523760\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [63/100]\n",
      " Avg Train Loss: 0.531315\n",
      "\n",
      " Epoch [64/100]\n",
      " Avg Train Loss: 0.530814\n",
      "\n",
      " Epoch [65/100]\n",
      " Avg Train Loss: 0.529068\n",
      "\n",
      " Epoch [66/100]\n",
      " Avg Train Loss: 0.527191\n",
      "\n",
      " Epoch [67/100]\n",
      " Avg Train Loss: 0.524818\n",
      "\n",
      " Epoch [68/100]\n",
      " Avg Train Loss: 0.525372\n",
      "\n",
      " Epoch [69/100]\n",
      " Avg Train Loss: 0.525574\n",
      "\n",
      " Epoch [70/100]\n",
      " Avg Train Loss: 0.531115\n",
      "\n",
      " Epoch [71/100]\n",
      " Avg Train Loss: 0.527739\n",
      "\n",
      " Epoch [72/100]\n",
      " Avg Train Loss: 0.527145\n",
      "\n",
      " Epoch [73/100]\n",
      " Avg Train Loss: 0.528596\n",
      "\n",
      " Epoch [74/100]\n",
      " Avg Train Loss: 0.524045\n",
      "\n",
      " Epoch [75/100]\n",
      " Avg Train Loss: 0.521827\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [76/100]\n",
      " Avg Train Loss: 0.528198\n",
      "\n",
      " Epoch [77/100]\n",
      " Avg Train Loss: 0.529861\n",
      "\n",
      " Epoch [78/100]\n",
      " Avg Train Loss: 0.521569\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [79/100]\n",
      " Avg Train Loss: 0.521429\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [80/100]\n",
      " Avg Train Loss: 0.522684\n",
      "\n",
      " Epoch [81/100]\n",
      " Avg Train Loss: 0.521467\n",
      "\n",
      " Epoch [82/100]\n",
      " Avg Train Loss: 0.523317\n",
      "\n",
      " Epoch [83/100]\n",
      " Avg Train Loss: 0.528939\n",
      "\n",
      " Epoch [84/100]\n",
      " Avg Train Loss: 0.527594\n",
      "\n",
      " Epoch [85/100]\n",
      " Avg Train Loss: 0.531871\n",
      "\n",
      " Epoch [86/100]\n",
      " Avg Train Loss: 0.523898\n",
      "\n",
      " Epoch [87/100]\n",
      " Avg Train Loss: 0.524471\n",
      "\n",
      " Epoch [88/100]\n",
      " Avg Train Loss: 0.527630\n",
      "\n",
      " Epoch [89/100]\n",
      " Avg Train Loss: 0.523655\n",
      "\n",
      " Epoch [90/100]\n",
      " Avg Train Loss: 0.528531\n",
      "\n",
      " Epoch [91/100]\n",
      " Avg Train Loss: 0.523251\n",
      "\n",
      " Epoch [92/100]\n",
      " Avg Train Loss: 0.523602\n",
      "\n",
      " Epoch [93/100]\n",
      " Avg Train Loss: 0.530663\n",
      "\n",
      " Epoch [94/100]\n",
      " Avg Train Loss: 0.521821\n",
      "\n",
      " Epoch [95/100]\n",
      " Avg Train Loss: 0.523851\n",
      "\n",
      " Epoch [96/100]\n",
      " Avg Train Loss: 0.521460\n",
      "\n",
      " Epoch [97/100]\n",
      " Avg Train Loss: 0.521794\n",
      "\n",
      " Epoch [98/100]\n",
      " Avg Train Loss: 0.520071\n",
      " âœ“ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [99/100]\n",
      " Avg Train Loss: 0.524317\n",
      "\n",
      " Epoch [100/100]\n",
      " Avg Train Loss: 0.522528\n",
      "\n",
      " Fine-tuning complete!\n",
      "\n",
      "Testing file: Bareilly-1H\n",
      "   â–¶ Building: BR02\n",
      "  Filled 9984 NaNs with median=0.1660\n",
      "   CVRMSE=0.0947, NRMSE=229.9386, MAE=0.0427\n",
      "\n",
      "Testing file: Mathura-1H\n",
      " BR02 not found, skipping...\n",
      "\n",
      "Testing complete!\n",
      "Results saved at: test_results_ft/test_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_file = '../Energy-TTM/config/tinyTimeMixers.json'\n",
    "with open(config_file, 'r') as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "# check device \n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# define TTMs model\n",
    "model = model_config(args).to(device)\n",
    "# Load pretrained model\n",
    "model.load_state_dict(torch.load('../Energy-TTM/Weights/energy_ttm.pth'))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=args[\"learning_rate\"],\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Fine-tune\n",
    "finetune(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    dataset_path='../Dataset/Forecasting',  #train_dataset_path\n",
    "    save_path='./finetuned_ttm',     #finetuned_model_path\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load best finetuned model\n",
    "model.load_state_dict(\n",
    "    torch.load(\"finetuned_ttm/best_model.pth\")\n",
    ")\n",
    "\n",
    "# Test\n",
    "test(args=args, model=model, criterion=criterion,dataset_path=\"../Dataset/Forecasting\",result_path=\"test_results_ft\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy-ttm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
