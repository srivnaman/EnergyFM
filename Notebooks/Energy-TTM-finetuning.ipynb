{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random \n",
    "\n",
    "\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"NCCL_IB_DISABLE\"] = \"1\"\n",
    "\n",
    "from time import time\n",
    "import math \n",
    "import tempfile \n",
    "import torch \n",
    "import pickle \n",
    "import logging \n",
    "import warnings\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, set_seed, EarlyStoppingCallback\n",
    "from torch.utils.data import ConcatDataset, Dataset, DataLoader\n",
    "\n",
    "\n",
    "from tsfm_public.models.tinytimemixer.configuration_tinytimemixer import TinyTimeMixerConfig\n",
    "# from tinytimemixer.modeling_tinytimemixer import TinyTimeMixerForPrediction\n",
    "from tsfm_public.models.tinytimemixer import TinyTimeMixerForPrediction\n",
    "from tsfm_public.toolkit.dataset import PretrainDFDataset, ForecastDFDataset\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n",
    "from tsfm_public.toolkit.util import select_by_index\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "SEED = 42\n",
    "set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# metrics used for evaluation\n",
    "def cal_cvrmse(pred, true, eps=1e-8):\n",
    "    pred = np.array(pred)\n",
    "    true = np.array(true)\n",
    "    return np.power(np.square(pred - true).sum() / pred.shape[0], 0.5) / (true.sum() / pred.shape[0] + eps)\n",
    "\n",
    "def cal_mae(pred, true):\n",
    "    pred = np.array(pred)\n",
    "    true = np.array(true)\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "def cal_nrmse(pred, true, eps=1e-8):\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    M = len(true) // 24\n",
    "    y_bar = np.mean(true)\n",
    "    NRMSE = 100 * (1/ (y_bar+eps)) * np.sqrt((1 / (24 * M)) * np.sum((true - pred) ** 2))\n",
    "    return NRMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def standardize_series(series, eps=1e-8):\n",
    "    mean = np.mean(series)\n",
    "    std = np.std(series)\n",
    "    standardized_series = (series - mean) / (std+eps)\n",
    "    return standardized_series, mean, std\n",
    "\n",
    "def unscale_predictions(predictions, mean, std, eps=1e-8):\n",
    "    return predictions * (std+eps) + mean\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, backcast_length, forecast_length, stride=1):\n",
    "        # Standardize the time series data\n",
    "        self.data, self.mean, self.std = standardize_series(data)\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data) - self.backcast_length - self.forecast_length) // self.stride + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start_index = index * self.stride\n",
    "        x = self.data[start_index : start_index + self.backcast_length]\n",
    "        y = self.data[start_index + self.backcast_length : start_index + self.backcast_length + self.forecast_length]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(args):\n",
    "\n",
    "    config = TinyTimeMixerConfig(\n",
    "        context_length=args[\"context_length\"],\n",
    "        patch_length=args[\"patch_length\"],\n",
    "        num_input_channels=args[\"num_input_channels\"],\n",
    "        patch_stride=args[\"patch_stride\"],\n",
    "        d_model=args[\"d_model\"],\n",
    "        num_layers=args[\"num_layers\"],\n",
    "        expansion_factor=args[\"expansion_factor\"],\n",
    "        dropout=args[\"dropout\"],\n",
    "        head_dropout=args[\"head_dropout\"],\n",
    "        mode=args[\"mode\"][0],\n",
    "        scaling=args[\"scaling\"],\n",
    "        prediction_length=args[\"prediction_length\"],\n",
    "        is_scaling=args[\"is_scaling\"],\n",
    "        gated_attn=args[\"gated_attn\"],\n",
    "        norm_mlp=args[\"norm_mlp\"],\n",
    "        self_attn=args[\"self_attn\"],\n",
    "        self_attn_heads=args[\"self_attn_heads\"],\n",
    "        use_positional_encoding=args[\"use_positional_encoding\"],\n",
    "        positional_encoding_type=args[\"positional_encoding_type\"],\n",
    "        loss=args[\"loss\"],\n",
    "        init_std=args[\"init_std\"],\n",
    "        post_init=args[\"post_init\"],\n",
    "        norm_eps=args[\"norm_eps\"],\n",
    "        adaptive_patching_levels=args[\"adaptive_patching_levels\"],\n",
    "        resolution_prefix_tuning=args[\"resolution_prefix_tuning\"],\n",
    "        frequency_token_vocab_size=args[\"frequency_token_vocab_size\"],\n",
    "        distribution_output=args[\"distribution_output\"],\n",
    "        num_parallel_samples=args[\"num_parallel_samples\"],\n",
    "        decoder_num_layers=args[\"decoder_num_layers\"],\n",
    "        decoder_d_model=args[\"decoder_d_model\"],\n",
    "        decoder_adaptive_patching_levels=args[\"decoder_adaptive_patching_levels\"],\n",
    "        decoder_raw_residual=args[\"decoder_raw_residual\"],\n",
    "        decoder_mode=args[\"decoder_mode\"],\n",
    "        use_decoder=args[\"use_decoder\"],\n",
    "        enable_forecast_channel_mixing=args[\"enable_forecast_channel_mixing\"],\n",
    "        fcm_gated_attn=args[\"fcm_gated_attn\"],\n",
    "        fcm_context_length=args[\"fcm_context_length\"],\n",
    "        fcm_use_mixer=args[\"fcm_use_mixer\"],\n",
    "        fcm_mix_layers=args[\"fcm_mix_layers\"],\n",
    "        fcm_prepend_past=args[\"fcm_prepend_past\"], \n",
    "        init_linear=args[\"init_linear\"],\n",
    "        init_embed=args[\"init_embed\"],\n",
    "\n",
    "    )\n",
    "\n",
    "    pretraining_model = TinyTimeMixerForPrediction(config)\n",
    "    return pretraining_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MH01</th>\n",
       "      <th>MH02</th>\n",
       "      <th>MH03</th>\n",
       "      <th>MH06</th>\n",
       "      <th>MH07</th>\n",
       "      <th>MH08</th>\n",
       "      <th>MH09</th>\n",
       "      <th>MH10</th>\n",
       "      <th>MH11</th>\n",
       "      <th>MH12</th>\n",
       "      <th>...</th>\n",
       "      <th>MH36</th>\n",
       "      <th>MH37</th>\n",
       "      <th>MH38</th>\n",
       "      <th>MH39</th>\n",
       "      <th>MH41</th>\n",
       "      <th>MH42</th>\n",
       "      <th>MH43</th>\n",
       "      <th>MH45</th>\n",
       "      <th>MH46</th>\n",
       "      <th>MH47</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-05-01 00:00:00</th>\n",
       "      <td>0.262</td>\n",
       "      <td>2.039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.437</td>\n",
       "      <td>1.915</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.283</td>\n",
       "      <td>2.484</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.342</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 01:00:00</th>\n",
       "      <td>0.309</td>\n",
       "      <td>2.036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.406</td>\n",
       "      <td>1.878</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.235</td>\n",
       "      <td>2.438</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.360</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 02:00:00</th>\n",
       "      <td>0.315</td>\n",
       "      <td>2.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.404</td>\n",
       "      <td>1.822</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.243</td>\n",
       "      <td>2.332</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.371</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 03:00:00</th>\n",
       "      <td>0.318</td>\n",
       "      <td>2.018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.470</td>\n",
       "      <td>1.844</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.272</td>\n",
       "      <td>2.322</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.314</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 04:00:00</th>\n",
       "      <td>0.324</td>\n",
       "      <td>2.045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.479</td>\n",
       "      <td>1.786</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.333</td>\n",
       "      <td>2.313</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 19:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 20:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 21:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 22:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-20 23:00:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15888 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      MH01   MH02  MH03   MH06   MH07   MH08   MH09   MH10  \\\n",
       "Timestamp                                                                    \n",
       "2019-05-01 00:00:00  0.262  2.039   NaN  0.437  1.915  0.302  0.283  2.484   \n",
       "2019-05-01 01:00:00  0.309  2.036   NaN  0.406  1.878  0.321  0.235  2.438   \n",
       "2019-05-01 02:00:00  0.315  2.018   NaN  0.404  1.822  0.321  0.243  2.332   \n",
       "2019-05-01 03:00:00  0.318  2.018   NaN  0.470  1.844  0.333  0.272  2.322   \n",
       "2019-05-01 04:00:00  0.324  2.045   NaN  0.479  1.786  0.322  0.333  2.313   \n",
       "...                    ...    ...   ...    ...    ...    ...    ...    ...   \n",
       "2021-02-20 19:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2021-02-20 20:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2021-02-20 21:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2021-02-20 22:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "2021-02-20 23:00:00    NaN    NaN   NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "                      MH11   MH12  ...  MH36  MH37  MH38  MH39   MH41   MH42  \\\n",
       "Timestamp                          ...                                         \n",
       "2019-05-01 00:00:00  0.788  0.342  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "2019-05-01 01:00:00  0.799  0.360  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "2019-05-01 02:00:00  0.815  0.371  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "2019-05-01 03:00:00  0.822  0.314  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "2019-05-01 04:00:00  0.592  0.250  ...   NaN   NaN   NaN   NaN    NaN    NaN   \n",
       "...                    ...    ...  ...   ...   ...   ...   ...    ...    ...   \n",
       "2021-02-20 19:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.153  0.195   \n",
       "2021-02-20 20:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.112  0.210   \n",
       "2021-02-20 21:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.097  0.130   \n",
       "2021-02-20 22:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.037  0.097   \n",
       "2021-02-20 23:00:00    NaN    NaN  ...   NaN   NaN   NaN   NaN  0.018  0.035   \n",
       "\n",
       "                      MH43   MH45  MH46  MH47  \n",
       "Timestamp                                      \n",
       "2019-05-01 00:00:00    NaN    NaN   NaN   NaN  \n",
       "2019-05-01 01:00:00    NaN    NaN   NaN   NaN  \n",
       "2019-05-01 02:00:00    NaN    NaN   NaN   NaN  \n",
       "2019-05-01 03:00:00    NaN    NaN   NaN   NaN  \n",
       "2019-05-01 04:00:00    NaN    NaN   NaN   NaN  \n",
       "...                    ...    ...   ...   ...  \n",
       "2021-02-20 19:00:00  0.282  0.140   NaN   NaN  \n",
       "2021-02-20 20:00:00  0.137  0.129   NaN   NaN  \n",
       "2021-02-20 21:00:00  0.202  0.179   NaN   NaN  \n",
       "2021-02-20 22:00:00  0.117  0.145   NaN   NaN  \n",
       "2021-02-20 23:00:00  0.011  0.073   NaN   NaN  \n",
       "\n",
       "[15888 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.read_parquet('../Dataset/Forecasting/Mathura-1H.parquet')\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    args,\n",
    "    model,\n",
    "    criterion,\n",
    "    dataset_path,\n",
    "    result_path,\n",
    "    device,\n",
    "    target_buildings=\"BR02\"   # can be \"BR02\", [\"BR02\",\"BR03\"], or \"all\"\n",
    "):\n",
    "\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    # Loop over parquet files\n",
    "    for file_name in os.listdir(dataset_path):\n",
    "\n",
    "        if not file_name.endswith(\".parquet\"):\n",
    "            continue\n",
    "\n",
    "        file_id = file_name.replace(\".parquet\", \"\")\n",
    "        file_path = os.path.join(dataset_path, file_name)\n",
    "\n",
    "        print(f\"Testing file: {file_id}\")\n",
    "\n",
    "        # Load parquet\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Determine target buildings\n",
    "        # ---------------------------------------------------\n",
    "        if target_buildings == \"all\":\n",
    "            buildings_to_test = list(df.columns)\n",
    "\n",
    "        elif isinstance(target_buildings, str):\n",
    "            buildings_to_test = [target_buildings]\n",
    "\n",
    "        elif isinstance(target_buildings, list):\n",
    "            buildings_to_test = target_buildings\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"target_buildings must be 'all', a string, or list\")\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Test each building column\n",
    "        # ---------------------------------------------------\n",
    "        for building_col in buildings_to_test:\n",
    "\n",
    "            if building_col not in df.columns:\n",
    "                print(f\" {building_col} not found in {file_id}, skipping...\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n   ▶ Building: {building_col}\")\n",
    "\n",
    "            # Extract series\n",
    "            energy_data = df[building_col].values.astype(np.float32)\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Fill NaNs with Median\n",
    "            # ---------------------------------------------------\n",
    "            nan_count = np.isnan(energy_data).sum()\n",
    "\n",
    "            if nan_count > 0:\n",
    "                median_val = np.nanmedian(energy_data)\n",
    "                energy_data = np.where(\n",
    "                    np.isnan(energy_data),\n",
    "                    median_val,\n",
    "                    energy_data\n",
    "                )\n",
    "\n",
    "                print(f\"  Filled {nan_count} NaNs with median={median_val:.4f}\")\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Check minimum length\n",
    "            # ---------------------------------------------------\n",
    "            min_required = args[\"context_length\"] + args[\"prediction_length\"]\n",
    "\n",
    "            if len(energy_data) < min_required:\n",
    "                print(\"   Too short, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Dataset creation\n",
    "            dataset = TimeSeriesDataset(\n",
    "                energy_data,\n",
    "                args[\"context_length\"],\n",
    "                args[\"prediction_length\"],\n",
    "                args[\"patch_stride\"]\n",
    "            )\n",
    "\n",
    "            if len(dataset) == 0:\n",
    "                print(\"   No samples, skipping...\")\n",
    "                continue\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            val_losses = []\n",
    "            y_true_test = []\n",
    "            y_pred_test = []\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Testing loop\n",
    "            # ---------------------------------------------------\n",
    "            for x_test, y_test in DataLoader(dataset, batch_size=1):\n",
    "\n",
    "                x_test = x_test.unsqueeze(-1).to(device)\n",
    "                y_test = y_test.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(x_test)\n",
    "                    forecast = output.prediction_outputs.squeeze(-1)\n",
    "\n",
    "                    loss = criterion(forecast, y_test)\n",
    "\n",
    "                    if torch.isnan(loss):\n",
    "                        continue\n",
    "\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "                    y_true_test.append(y_test.cpu().numpy())\n",
    "                    y_pred_test.append(forecast.cpu().numpy())\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            #  Skip empty results\n",
    "            # ---------------------------------------------------\n",
    "            if len(y_true_test) == 0:\n",
    "                print(\"   No predictions collected, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # Combine predictions\n",
    "            y_true = np.concatenate(y_true_test, axis=0)\n",
    "            y_pred = np.concatenate(y_pred_test, axis=0)\n",
    "\n",
    "            # Unscale\n",
    "            y_pred_unscaled = unscale_predictions(y_pred, dataset.mean, dataset.std)\n",
    "            y_true_unscaled = unscale_predictions(y_true, dataset.mean, dataset.std)\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            #  Metrics\n",
    "            # ---------------------------------------------------\n",
    "            cvrmse = cal_cvrmse(y_pred_unscaled, y_true_unscaled)\n",
    "            nrmse  = cal_nrmse(y_pred_unscaled, y_true_unscaled)\n",
    "            mae    = cal_mae(y_pred_unscaled, y_true_unscaled)\n",
    "\n",
    "            avg_loss = np.mean(val_losses)\n",
    "\n",
    "            print(f\"  CVRMSE={cvrmse:.4f}, NRMSE={nrmse:.4f}, MAE={mae:.4f}\")\n",
    "\n",
    "            # Save row\n",
    "            res.append([\n",
    "                file_id,\n",
    "                building_col,\n",
    "                cvrmse,\n",
    "                nrmse,\n",
    "                mae,\n",
    "                avg_loss\n",
    "            ])\n",
    "\n",
    "    # ---------------------------------------------------\n",
    "    # Save Results\n",
    "    # ---------------------------------------------------\n",
    "    columns = [\"Dataset\", \"Building\", \"CVRMSE\", \"NRMSE\", \"MAE\", \"Avg_Test_Loss\"]\n",
    "\n",
    "    result_df = pd.DataFrame(res, columns=columns)\n",
    "\n",
    "    result_csv = os.path.join(result_path, \"test_results.csv\")\n",
    "    result_df.to_csv(result_csv, index=False)\n",
    "\n",
    "    print(\"\\n Testing complete!\")\n",
    "    print(\" Results saved at:\", result_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(\n",
    "    args,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dataset_path,\n",
    "    save_path,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fine-tune a pretrained TinyTimeMixer (TTM) model.\n",
    "\n",
    "    Args:\n",
    "        args (dict): config dictionary\n",
    "        model (nn.Module): pretrained TTM model\n",
    "        criterion (loss): training loss (e.g. MSE)\n",
    "        optimizer (torch.optim): optimizer\n",
    "        dataset_path (str): path containing parquet files\n",
    "        save_path (str): directory to save finetuned model\n",
    "        device (torch.device)\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    model.train()\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    print(\"\\n Starting fine-tuning...\")\n",
    "\n",
    "    for epoch in range(args[\"num_epochs\"]):\n",
    "\n",
    "        epoch_losses = []\n",
    "\n",
    "        print(f\"\\n Epoch [{epoch+1}/{args['num_epochs']}]\")\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Loop over parquet files\n",
    "        # ---------------------------------------------------\n",
    "        for file_name in os.listdir(dataset_path):\n",
    "\n",
    "            if not file_name.endswith(\".parquet\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(dataset_path, file_name)\n",
    "            df = pd.read_parquet(file_path)\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Loop over building columns\n",
    "            # ---------------------------------------------------\n",
    "            for building_col in df.columns:\n",
    "\n",
    "                if not pd.api.types.is_numeric_dtype(df[building_col]):\n",
    "                    continue\n",
    "\n",
    "                energy_data = df[building_col].values.astype(np.float32)\n",
    "\n",
    "                # Fill NaNs with median\n",
    "                if np.isnan(energy_data).any():\n",
    "                    energy_data = np.nan_to_num(\n",
    "                        energy_data,\n",
    "                        nan=np.nanmedian(energy_data)\n",
    "                    )\n",
    "\n",
    "                min_required = (\n",
    "                    args[\"context_length\"] + args[\"prediction_length\"]\n",
    "                )\n",
    "\n",
    "                if len(energy_data) < min_required:\n",
    "                    continue\n",
    "\n",
    "                dataset = TimeSeriesDataset(\n",
    "                    energy_data,\n",
    "                    args[\"context_length\"],\n",
    "                    args[\"prediction_length\"],\n",
    "                    args[\"patch_stride\"],\n",
    "                )\n",
    "\n",
    "                if len(dataset) == 0:\n",
    "                    continue\n",
    "\n",
    "                loader = DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=args[\"batch_size\"],\n",
    "                    shuffle=True\n",
    "                )\n",
    "\n",
    "                # ---------------------------------------------------\n",
    "                # Training loop\n",
    "                # ---------------------------------------------------\n",
    "                for x, y in loader:\n",
    "\n",
    "                    x = x.unsqueeze(-1).to(device)\n",
    "                    y = y.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    output = model(x)\n",
    "                    forecast = output.prediction_outputs.squeeze(-1)\n",
    "\n",
    "                    loss = criterion(forecast, y)\n",
    "\n",
    "                    if torch.isnan(loss):\n",
    "                        continue\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_losses.append(loss.item())\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Epoch summary\n",
    "        # ---------------------------------------------------\n",
    "        if len(epoch_losses) == 0:\n",
    "            print(\" No valid batches in this epoch.\")\n",
    "            continue\n",
    "\n",
    "        avg_epoch_loss = np.mean(epoch_losses)\n",
    "        print(f\" Avg Train Loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "        # Save best checkpoint\n",
    "        # ---------------------------------------------------\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            ckpt_path = os.path.join(save_path, \"best_model.pth\")\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "            print(f\" ✓ Saved best model to {ckpt_path}\")\n",
    "\n",
    "    print(\"\\n Fine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/naman/Final-EFM/EnergyFM/Notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:p-319123:t-132784061216576:modeling_tinytimemixer.py:__init__:Disabling adaptive patching at level 2. Either increase d_model or reduce adaptive_patching_levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's parameter count is: 28858\n",
      "Testing file: Bareilly-1H\n",
      "\n",
      "   ▶ Building: BR02\n",
      "  Filled 9984 NaNs with median=0.1660\n",
      "  CVRMSE=0.0974, NRMSE=236.3778, MAE=0.0428\n",
      "Testing file: Mathura-1H\n",
      " BR02 not found in Mathura-1H, skipping...\n",
      "\n",
      " Testing complete!\n",
      " Results saved at: test_results/test_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config_file = '../Energy-TTM/config/tinyTimeMixers.json'\n",
    "with open(config_file, 'r') as f:\n",
    "    args = json.load(f)\n",
    "\n",
    "# check device \n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# define TTMs model\n",
    "model = model_config(args).to(device)\n",
    "# Load pretrained model\n",
    "model.load_state_dict(torch.load('../Energy-TTM/Weights/energy_ttm.pth'))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=args[\"learning_rate\"],\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Fine-tune\n",
    "finetune(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    dataset_path='../Dataset/Forecasting',  #train_dataset_path\n",
    "    save_path='./finetuned_ttm',     #finetuned_model_path\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load best finetuned model\n",
    "model.load_state_dict(\n",
    "    torch.load(\"finetuned_ttm/best_model.pth\")\n",
    ")\n",
    "\n",
    "# Test\n",
    "test(args=args, model=model, criterion=criterion,dataset_path=\"../Dataset/Forecasting\",result_path=\"test_results_ft\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting fine-tuning...\n",
      "\n",
      " Epoch [1/100]\n",
      " Avg Train Loss: 0.544261\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [2/100]\n",
      " Avg Train Loss: 0.543707\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [3/100]\n",
      " Avg Train Loss: 0.542355\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [4/100]\n",
      " Avg Train Loss: 0.544567\n",
      "\n",
      " Epoch [5/100]\n",
      " Avg Train Loss: 0.549365\n",
      "\n",
      " Epoch [6/100]\n",
      " Avg Train Loss: 0.541528\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [7/100]\n",
      " Avg Train Loss: 0.538067\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [8/100]\n",
      " Avg Train Loss: 0.542100\n",
      "\n",
      " Epoch [9/100]\n",
      " Avg Train Loss: 0.535901\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [10/100]\n",
      " Avg Train Loss: 0.542535\n",
      "\n",
      " Epoch [11/100]\n",
      " Avg Train Loss: 0.535038\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [12/100]\n",
      " Avg Train Loss: 0.535902\n",
      "\n",
      " Epoch [13/100]\n",
      " Avg Train Loss: 0.538425\n",
      "\n",
      " Epoch [14/100]\n",
      " Avg Train Loss: 0.536919\n",
      "\n",
      " Epoch [15/100]\n",
      " Avg Train Loss: 0.550819\n",
      "\n",
      " Epoch [16/100]\n",
      " Avg Train Loss: 0.535419\n",
      "\n",
      " Epoch [17/100]\n",
      " Avg Train Loss: 0.533450\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [18/100]\n",
      " Avg Train Loss: 0.531999\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [19/100]\n",
      " Avg Train Loss: 0.532664\n",
      "\n",
      " Epoch [20/100]\n",
      " Avg Train Loss: 0.532667\n",
      "\n",
      " Epoch [21/100]\n",
      " Avg Train Loss: 0.538449\n",
      "\n",
      " Epoch [22/100]\n",
      " Avg Train Loss: 0.532237\n",
      "\n",
      " Epoch [23/100]\n",
      " Avg Train Loss: 0.537975\n",
      "\n",
      " Epoch [24/100]\n",
      " Avg Train Loss: 0.532933\n",
      "\n",
      " Epoch [25/100]\n",
      " Avg Train Loss: 0.532962\n",
      "\n",
      " Epoch [26/100]\n",
      " Avg Train Loss: 0.528836\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [27/100]\n",
      " Avg Train Loss: 0.539762\n",
      "\n",
      " Epoch [28/100]\n",
      " Avg Train Loss: 0.528193\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [29/100]\n",
      " Avg Train Loss: 0.532428\n",
      "\n",
      " Epoch [30/100]\n",
      " Avg Train Loss: 0.533427\n",
      "\n",
      " Epoch [31/100]\n",
      " Avg Train Loss: 0.530666\n",
      "\n",
      " Epoch [32/100]\n",
      " Avg Train Loss: 0.529556\n",
      "\n",
      " Epoch [33/100]\n",
      " Avg Train Loss: 0.526659\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [34/100]\n",
      " Avg Train Loss: 0.527894\n",
      "\n",
      " Epoch [35/100]\n",
      " Avg Train Loss: 0.527520\n",
      "\n",
      " Epoch [36/100]\n",
      " Avg Train Loss: 0.531102\n",
      "\n",
      " Epoch [37/100]\n",
      " Avg Train Loss: 0.528809\n",
      "\n",
      " Epoch [38/100]\n",
      " Avg Train Loss: 0.531205\n",
      "\n",
      " Epoch [39/100]\n",
      " Avg Train Loss: 0.529809\n",
      "\n",
      " Epoch [40/100]\n",
      " Avg Train Loss: 0.525510\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [41/100]\n",
      " Avg Train Loss: 0.534986\n",
      "\n",
      " Epoch [42/100]\n",
      " Avg Train Loss: 0.533108\n",
      "\n",
      " Epoch [43/100]\n",
      " Avg Train Loss: 0.528342\n",
      "\n",
      " Epoch [44/100]\n",
      " Avg Train Loss: 0.530614\n",
      "\n",
      " Epoch [45/100]\n",
      " Avg Train Loss: 0.527044\n",
      "\n",
      " Epoch [46/100]\n",
      " Avg Train Loss: 0.526095\n",
      "\n",
      " Epoch [47/100]\n",
      " Avg Train Loss: 0.528399\n",
      "\n",
      " Epoch [48/100]\n",
      " Avg Train Loss: 0.531913\n",
      "\n",
      " Epoch [49/100]\n",
      " Avg Train Loss: 0.531799\n",
      "\n",
      " Epoch [50/100]\n",
      " Avg Train Loss: 0.526842\n",
      "\n",
      " Epoch [51/100]\n",
      " Avg Train Loss: 0.527220\n",
      "\n",
      " Epoch [52/100]\n",
      " Avg Train Loss: 0.527222\n",
      "\n",
      " Epoch [53/100]\n",
      " Avg Train Loss: 0.525884\n",
      "\n",
      " Epoch [54/100]\n",
      " Avg Train Loss: 0.532776\n",
      "\n",
      " Epoch [55/100]\n",
      " Avg Train Loss: 0.533978\n",
      "\n",
      " Epoch [56/100]\n",
      " Avg Train Loss: 0.530244\n",
      "\n",
      " Epoch [57/100]\n",
      " Avg Train Loss: 0.532156\n",
      "\n",
      " Epoch [58/100]\n",
      " Avg Train Loss: 0.530205\n",
      "\n",
      " Epoch [59/100]\n",
      " Avg Train Loss: 0.531107\n",
      "\n",
      " Epoch [60/100]\n",
      " Avg Train Loss: 0.527497\n",
      "\n",
      " Epoch [61/100]\n",
      " Avg Train Loss: 0.523524\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [62/100]\n",
      " Avg Train Loss: 0.522002\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [63/100]\n",
      " Avg Train Loss: 0.527464\n",
      "\n",
      " Epoch [64/100]\n",
      " Avg Train Loss: 0.525546\n",
      "\n",
      " Epoch [65/100]\n",
      " Avg Train Loss: 0.522447\n",
      "\n",
      " Epoch [66/100]\n",
      " Avg Train Loss: 0.524987\n",
      "\n",
      " Epoch [67/100]\n",
      " Avg Train Loss: 0.526569\n",
      "\n",
      " Epoch [68/100]\n",
      " Avg Train Loss: 0.524607\n",
      "\n",
      " Epoch [69/100]\n",
      " Avg Train Loss: 0.537157\n",
      "\n",
      " Epoch [70/100]\n",
      " Avg Train Loss: 0.523777\n",
      "\n",
      " Epoch [71/100]\n",
      " Avg Train Loss: 0.524306\n",
      "\n",
      " Epoch [72/100]\n",
      " Avg Train Loss: 0.522604\n",
      "\n",
      " Epoch [73/100]\n",
      " Avg Train Loss: 0.524884\n",
      "\n",
      " Epoch [74/100]\n",
      " Avg Train Loss: 0.519265\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [75/100]\n",
      " Avg Train Loss: 0.525597\n",
      "\n",
      " Epoch [76/100]\n",
      " Avg Train Loss: 0.531410\n",
      "\n",
      " Epoch [77/100]\n",
      " Avg Train Loss: 0.523798\n",
      "\n",
      " Epoch [78/100]\n",
      " Avg Train Loss: 0.529517\n",
      "\n",
      " Epoch [79/100]\n",
      " Avg Train Loss: 0.523223\n",
      "\n",
      " Epoch [80/100]\n",
      " Avg Train Loss: 0.521706\n",
      "\n",
      " Epoch [81/100]\n",
      " Avg Train Loss: 0.522143\n",
      "\n",
      " Epoch [82/100]\n",
      " Avg Train Loss: 0.527323\n",
      "\n",
      " Epoch [83/100]\n",
      " Avg Train Loss: 0.521451\n",
      "\n",
      " Epoch [84/100]\n",
      " Avg Train Loss: 0.526032\n",
      "\n",
      " Epoch [85/100]\n",
      " Avg Train Loss: 0.523608\n",
      "\n",
      " Epoch [86/100]\n",
      " Avg Train Loss: 0.528883\n",
      "\n",
      " Epoch [87/100]\n",
      " Avg Train Loss: 0.520228\n",
      "\n",
      " Epoch [88/100]\n",
      " Avg Train Loss: 0.522095\n",
      "\n",
      " Epoch [89/100]\n",
      " Avg Train Loss: 0.529175\n",
      "\n",
      " Epoch [90/100]\n",
      " Avg Train Loss: 0.523034\n",
      "\n",
      " Epoch [91/100]\n",
      " Avg Train Loss: 0.517892\n",
      " ✓ Saved best model to ./finetuned_ttm/best_model.pth\n",
      "\n",
      " Epoch [92/100]\n",
      " Avg Train Loss: 0.518484\n",
      "\n",
      " Epoch [93/100]\n",
      " Avg Train Loss: 0.520481\n",
      "\n",
      " Epoch [94/100]\n",
      " Avg Train Loss: 0.533377\n",
      "\n",
      " Epoch [95/100]\n",
      " Avg Train Loss: 0.521869\n",
      "\n",
      " Epoch [96/100]\n",
      " Avg Train Loss: 0.522946\n",
      "\n",
      " Epoch [97/100]\n",
      " Avg Train Loss: 0.525654\n",
      "\n",
      " Epoch [98/100]\n",
      " Avg Train Loss: 0.520289\n",
      "\n",
      " Epoch [99/100]\n",
      " Avg Train Loss: 0.521796\n",
      "\n",
      " Epoch [100/100]\n",
      " Avg Train Loss: 0.520821\n",
      "\n",
      " Fine-tuning complete!\n",
      "Testing file: Bareilly-1H\n",
      "\n",
      "   ▶ Building: BR02\n",
      "  Filled 9984 NaNs with median=0.1660\n",
      "  CVRMSE=0.0948, NRMSE=230.2339, MAE=0.0427\n",
      "Testing file: Mathura-1H\n",
      " BR02 not found in Mathura-1H, skipping...\n",
      "\n",
      " Testing complete!\n",
      " Results saved at: test_results_ft/test_results.csv\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy-ttm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
